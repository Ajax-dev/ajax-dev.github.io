---
id: 9gw75nmetumwb4lj71vfjmy
title: BigO
desc: ''
updated: 1669736169458
created: 1669736158702
---
Big O notation or 'computational complexity theory' focuses on classification of computational problems based on their resource usage.

*Big O notation is a relative representation of the complexity of an algorithm*

- **relative** - you can't compare an arithmetic multiplaction algorithm to a list sorting algorithm, but you could compare 2 arithmetic algorithms.
- **representation** - in its simplest form it reduces comparison between algorithms to single variable. That variable is chosen based on observations or assumptions, i.e. sorting algorithms are typically compared based on comparison operators.
- **complexity** - if it takes me one second to sort 10,000 elements, how long will it take me to sort one million.

O(1) has the least complexity.

O(log(n)) is more complex than O(1) but less complex than polynomials. Usually divide and conquer algorithms.

Complexity of polynomials increases as exponent increases.